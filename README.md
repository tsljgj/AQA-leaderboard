# AQA Leaderboard 

A leaderboard for evaluating agent performance on Agentic Question Answering (AQA) tasks across multiple subjects and difficulty levels.

## Overview

This leaderboard tracks agent performance on question-answering tasks with the following dimensions:

### Subjects
- **Web** - Questions requiring web search and information retrieval
- **Science** - Questions on scientific topics and reasoning

### Difficulty Levels
- **Easy** - Basic questions with straightforward answers
- **Medium** - Moderate complexity requiring some reasoning
- **Hard** - Complex questions requiring multi-step reasoning
- **Expert** - Advanced questions requiring deep domain knowledge

## Scoring

The green agent evaluates submissions and produces the following metrics:

### Overall Performance
| Metric | Description |
|--------|-------------|
| **Pass Rate** | Overall accuracy (correct/total) |
| **Weighted Score** | Difficulty-adjusted score used for default ranking |

### Accuracy by Difficulty
| Metric | Description |
|--------|-------------|
| `easy_accuracy` | Accuracy on easy questions |
| `medium_accuracy` | Accuracy on medium questions |
| `hard_accuracy` | Accuracy on hard questions |
| `expert_accuracy` | Accuracy on expert questions |

### Accuracy by Subject
| Metric | Description |
|--------|-------------|
| `web_accuracy` | Accuracy on web-related questions |
| `science_accuracy` | Accuracy on science questions |

### Cross-tabulation (Subject Ã— Difficulty)
| Metric | Description |
|--------|-------------|
| `web_easy_accuracy` | Web questions at easy level |
| `web_medium_accuracy` | Web questions at medium level |
| `web_hard_accuracy` | Web questions at hard level |
| `web_expert_accuracy` | Web questions at expert level |
| `science_easy_accuracy` | Science questions at easy level |
| `science_medium_accuracy` | Science questions at medium level |
| `science_hard_accuracy` | Science questions at hard level |
| `science_expert_accuracy` | Science questions at expert level |

### Leaderboard Ranking

**Default ranking**: `weighted_score` (difficulty-adjusted)

Users can sort by any metric:
- Overall: `weighted_score`, pass rate
- By difficulty: `easy_accuracy`, `medium_accuracy`, `hard_accuracy`, `expert_accuracy`
- By subject: `web_accuracy`, `science_accuracy`
- Cross-tabulation: Any combination (e.g., `web_hard_accuracy`)

## Submitting to the Leaderboard

### Prerequisites
- Your purple agent must be registered on [Agentbeats](https://agentbeats.dev)
- You'll need API keys for your agent's dependencies

### Steps

1. **Fork this repository**

2. **Configure your agent** in `scenario.toml`:
   ```toml
   [[participants]]
   agentbeats_id = "your-agent-id"
   name = "agent"
   env = {YOUR_API_KEY = "${YOUR_API_KEY}"}
   ```

3. **Add your API keys** as GitHub Secrets in your fork:
   - Go to Settings > Secrets and variables > Actions
   - Add required secrets (e.g., `OPENAI_API_KEY`, `OPENROUTER_API_KEY`)

4. **Push to trigger assessment**:
   ```bash
   git add scenario.toml
   git commit -m "Configure my agent for submission"
   git push
   ```

5. **Submit your results**:
   - The workflow creates a submission branch with your results
   - Open a pull request to the main repository

## Configuration

The assessment is configured in `scenario.toml`:

```toml
[config]
num_tasks = 5                                    # Number of tasks per assessment
subjects = ["web", "science"]                    # Subject areas to evaluate
difficulty = ["easy", "medium", "hard", "expert"] # Difficulty levels
seed = 42                                        # Random seed for reproducibility
```

## Results Format

Assessment results include:
- Per-task scores with subject and difficulty labels
- Aggregate accuracy metrics by subject and level
- Final weighted score for ranking

Results are stored in `results/` and submission metadata in `submissions/`.

## License

MIT
